{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "compression-train.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abel-bernabeu/autoencoder/blob/master/train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "myODRbvAf3EK",
        "colab_type": "text"
      },
      "source": [
        "# Description\n",
        "This notebook is a template for training autoencoders for different problems."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bFwt4ewqf3EN",
        "colab_type": "text"
      },
      "source": [
        "# Problem specifics\n",
        "\n",
        "This section contains all the specifics that need to be taylored for a given problem solved with an autoencoder.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rWvnxFWQlPiY",
        "colab_type": "text"
      },
      "source": [
        "## Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TdAi7hZDf3EO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hparams = {\n",
        "    'batch_size': 8,\n",
        "    'device': 'cuda',\n",
        "    'max_dataset_size': 20,\n",
        "    'train_dataset_size':10,\n",
        "    'test_dataset_size': 10,\n",
        "    'num_epochs': 1000,\n",
        "    'num_workers': 4,\n",
        "    'params' : \"./params/mock-compressor.pt\",\n",
        "    'continue_with_best_model' : False,\n",
        "    'checkpointing_freq' : 20\n",
        "}"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBBpNtoZf3EY",
        "colab_type": "text"
      },
      "source": [
        "## Dataset transforms"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TRfQvUDLf3EZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torchvision.transforms as transforms\n",
        "\n",
        "train_input_transform = transforms.Compose([transforms.ToTensor(),])\n",
        "train_output_transform = transforms.Compose([transforms.ToTensor()])\n",
        "test_input_transform = transforms.Compose([transforms.ToTensor(),])\n",
        "test_output_transform = transforms.Compose([transforms.ToTensor()])"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ntf1Z4_Yf3Eg",
        "colab_type": "text"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8rzeY6B_f3Eg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import autoencoder.models\n",
        "\n",
        "model = autoencoder.models.MockCompressor(input_width=224, input_height=224)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kBIbWHHrf3El",
        "colab_type": "text"
      },
      "source": [
        "# Training generics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ysBd-c-DlYA8",
        "colab_type": "text"
      },
      "source": [
        "## Visualization setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "AKr9CMXqSDj2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 821
        },
        "outputId": "274c0441-b9c6-4994-97aa-661878146181"
      },
      "source": [
        "# Launch Tensorboard\n",
        "#!rm runs -rf # Uncomment to delete all the previous Tensorboard runs\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir runs\n",
        "%reload_ext tensorboard"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.lib.display.IFrame at 0x7fdcc269abe0>"
            ],
            "text/html": [
              "\n",
              "        <iframe\n",
              "            width=\"100%\"\n",
              "            height=\"800\"\n",
              "            src=\"http://localhost:6006\"\n",
              "            frameborder=\"0\"\n",
              "            allowfullscreen\n",
              "        ></iframe>\n",
              "        "
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZCk3Yijgf3Er",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create directory for training checkpoints\n",
        "#!rm params -rf # Uncomment to delete all the checkpoints\n",
        "%mkdir -p params"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "THBPV92Kf3Ex",
        "colab_type": "text"
      },
      "source": [
        "## Kick off"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5EXszz0fSDkK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "outputId": "caf2e6f8-78c3-4090-f79d-58e9327ca8ec"
      },
      "source": [
        "import torch.optim as optim\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import torchvision.utils\n",
        "import numpy as np\n",
        "import autoencoder.datasets\n",
        "import datetime\n",
        "import os"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "AEuC5t4iM_CC",
        "colab": {}
      },
      "source": [
        "writer = SummaryWriter('')"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_1IKGDMxCOGL",
        "colab": {}
      },
      "source": [
        "# Create a crops dataset\n",
        "crops = autoencoder.datasets.CropsDataset(\"./data/image_dataset_part-a\", 224, 224, subset_size=hparams['max_dataset_size'], assume_fixed_size=True)\n",
        "\n",
        "# Show a few crops\n",
        "few_crops = [ transforms.ToTensor()(crop[0]) for crop in [crops[index] for index in range(16)]]\n",
        "grid = torchvision.utils.make_grid(few_crops, nrow=4)\n",
        "writer.add_image(\"1) a few crops\", grid)\n",
        "writer.flush()"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NaXiCozCEzB0",
        "colab": {}
      },
      "source": [
        "# Random split the original dataset in train and test datasets\n",
        "train_crops, test_crops = torch.utils.data.random_split(crops, [hparams['train_dataset_size'], hparams['test_dataset_size'],])"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "L5Rlhxt2SaaX",
        "colab": {}
      },
      "source": [
        "# Wrap the train samples with an XYDimsDataset\n",
        "train_xydims_samples = autoencoder.datasets.XYDimsDataset(train_input_transform, train_output_transform, dataset=train_crops)\n",
        "\n",
        "# Show x from a few train samples\n",
        "few_train_x = [ sample[0] for sample in [train_xydims_samples[index] for index in range(4)] ]\n",
        "grid = torchvision.utils.make_grid(few_train_x, nrow=4)\n",
        "writer.add_image(\"2) x from a few train samples\", grid)\n",
        "writer.flush()\n",
        "\n",
        "# Show y from a few train samples\n",
        "few_train_y = [ sample[1] for sample in [train_xydims_samples[index] for index in range(4)] ]\n",
        "grid = torchvision.utils.make_grid(few_train_y, nrow=4)\n",
        "writer.add_image(\"3) y from a few train samples\", grid)\n",
        "writer.flush()"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qm1qG67AUQx3",
        "colab": {}
      },
      "source": [
        "# Wrap the test samples with an XYDimsDataset\n",
        "test_xydims_samples = autoencoder.datasets.XYDimsDataset(test_input_transform, test_output_transform, dataset=test_crops)\n",
        "\n",
        "# Show x from a few test samples\n",
        "few_test_x = [ sample[0] for sample in [test_xydims_samples[index] for index in range(4)] ]\n",
        "grid = torchvision.utils.make_grid(few_test_x, nrow=4)\n",
        "writer.add_image(\"4) x from a few test samples\", grid)\n",
        "writer.flush()\n",
        "\n",
        "# Show y from a few train samples\n",
        "few_test_y = [ sample[1] for sample in [test_xydims_samples[index] for index in range(4)] ]\n",
        "grid = torchvision.utils.make_grid(few_test_y, nrow=4)\n",
        "writer.add_image(\"5) y from a few test samples\", grid)\n",
        "writer.flush()"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-MxSoKSUnXgS",
        "colab": {}
      },
      "source": [
        "# Create data loaders\n",
        "train_loader = torch.utils.data.DataLoader(train_xydims_samples, batch_size=hparams['batch_size'], shuffle=True, num_workers=hparams['num_workers'])\n",
        "test_loader = torch.utils.data.DataLoader(test_xydims_samples, batch_size=hparams['batch_size'], shuffle=False, num_workers=hparams['num_workers'])"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jOW0NNlhfHA5",
        "colab": {}
      },
      "source": [
        "# Some auxilary functions for the training loop\n",
        "def train_epoch(train_loader, model, optimizer, criterion, hparams):\n",
        "    np.random.seed(datetime.datetime.now().microsecond)\n",
        "    model.train()\n",
        "    device = hparams['device']\n",
        "    losses = []\n",
        "    for data, target, _, _ in train_loader:\n",
        "        data = data.to(device)\n",
        "        target = target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        losses.append(loss.item())\n",
        "    return np.mean(losses)\n",
        "\n",
        "def test_epoch(test_loader, model, criterion, hparams):\n",
        "    np.random.seed(0)\n",
        "    model.eval()\n",
        "    device = hparams['device']\n",
        "    eval_losses = []\n",
        "    with torch.no_grad():\n",
        "        for data, target, _, _ in test_loader:\n",
        "            data = data.to(device)\n",
        "            target = target.to(device)\n",
        "            output = model(data)\n",
        "            eval_losses.append(criterion(output, target).item())\n",
        "    return np.mean(eval_losses)\n",
        "\n",
        "def inference(model, inputs_list):\n",
        "    \"\"\"\n",
        "    Do an inference with the model for each input tensor from the provided list and\n",
        "    return a list with the inference results\n",
        "    \"\"\"\n",
        "    result = []\n",
        "    for x in inputs_list:\n",
        "        num_channels = x.shape[0]\n",
        "        height = x.shape[1]\n",
        "        width = x.shape[2]\n",
        "        single_element_batch = x.clone().detach().reshape(1, num_channels, height, width)\n",
        "        single_element_batch = single_element_batch.to(hparams['device'])\n",
        "        model.to(hparams['device'])\n",
        "        model.eval()\n",
        "        output = model(single_element_batch)\n",
        "        output = output.reshape(num_channels, height, width)\n",
        "        result.append(output)\n",
        "    return result"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Z7U6yTCe7xqe",
        "colab": {}
      },
      "source": [
        "# Move few_test_x to the same device where the inferences will be left\n",
        "for index in range(len(few_test_x)):\n",
        "  few_test_x[index] = few_test_x[index].to(hparams['device'])\n",
        "\n",
        "# Move few_test_y to the same device where the inferences will be left\n",
        "for index in range(len(few_train_y)):\n",
        "  few_train_y[index] = few_train_y[index].to(hparams['device'])\n",
        "\n",
        "# Move few_train_x to the same device where the inferences will be left\n",
        "for index in range(len(few_train_x)):\n",
        "  few_train_x[index] = few_train_x[index].to(hparams['device'])\n",
        "\n",
        "# Move few_train_y to the same device where the inferences will be left\n",
        "for index in range(len(few_test_y)):\n",
        "  few_test_y[index] = few_test_y[index].to(hparams['device'])"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UF02sWDZf3Fa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Instantiate optimer and loss\n",
        "optimizer = optim.Adam(model.parameters(), weight_decay=1e-4)\n",
        "criterion = nn.MSELoss()"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yWsFbEZ5f3Fd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Move model to device\n",
        "model = model.to(hparams['device'])"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "K-TS737bhbTg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f50619d7-ad8e-4217-b793-2ad1397d6645"
      },
      "source": [
        "# Restore model and optimizer from previous checkpoint or create new checkpoint from scratch\n",
        "if os.path.isfile(hparams['params']):\n",
        "    print(\"Restoring from previous checkpoint\")\n",
        "    checkpoint = torch.load(hparams['params'])    \n",
        "    if hparams['continue_with_best_model']:\n",
        "        model.load_state_dict(checkpoint['best_model'])\n",
        "    else:\n",
        "        model.load_state_dict(checkpoint['last_model'])        \n",
        "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "else:\n",
        "    next_epoch = 0\n",
        "    best_model_params = model.state_dict()\n",
        "    checkpoint = {\n",
        "        'epoch' : 0,\n",
        "        'best_train_loss': None,\n",
        "        'best_model': model.state_dict(),\n",
        "        'last_model': model.state_dict(),\n",
        "        'optimizer' : optimizer.state_dict()\n",
        "    }\n",
        "\n",
        "# Run a number of training epochs\n",
        "start = checkpoint['epoch']\n",
        "end = hparams['num_epochs']\n",
        "\n",
        "if start < end - 1 or checkpoint['best_train_loss'] is None:\n",
        "    \n",
        "    try:\n",
        "        for epoch in range(start, end):\n",
        "\n",
        "            train_loss = train_epoch(train_loader, model, optimizer, criterion, hparams)\n",
        "            \n",
        "            test_loss = test_epoch(test_loader, model, criterion, hparams)            \n",
        "\n",
        "            if epoch == hparams['num_epochs'] - 1 or epoch % hparams['checkpointing_freq'] == 0:\n",
        "\n",
        "                print('Saving checkpoint for epoch ' + str(epoch))\n",
        "                checkpoint['epoch'] = epoch\n",
        "\n",
        "                if checkpoint['best_train_loss'] is None or train_loss < checkpoint['best_train_loss']:\n",
        "                    print('New best model found!')                \n",
        "                    checkpoint['best_train_loss'] = train_loss\n",
        "                    checkpoint['best_model'] = model.state_dict()\n",
        "\n",
        "                checkpoint['last_model'] = model.state_dict()\n",
        "\n",
        "                checkpoint['optimizer'] = optimizer.state_dict()\n",
        "                \n",
        "                torch.save(checkpoint, hparams['params'])\n",
        "\n",
        "            writer.add_scalar(\"train loss\", train_loss, global_step=epoch)\n",
        "                        \n",
        "            writer.add_scalar(\"test loss\", test_loss, global_step=epoch)\n",
        "\n",
        "            # Show inferences with a few training samples\n",
        "            few_train_y_hat = inference(model, few_train_x)\n",
        "            grid = torchvision.utils.make_grid(few_train_y + few_train_x + few_train_y_hat, nrow=4)\n",
        "            writer.add_image(\"a few train samples, one column per sample in (y, x, y_hat) format\", grid, global_step=epoch)\n",
        "\n",
        "            # Show inferences with a few test samples\n",
        "            few_test_y_hat = inference(model, few_test_x)\n",
        "            grid = torchvision.utils.make_grid(few_test_y + few_test_x + few_test_y_hat, nrow=4)\n",
        "            writer.add_image(\"a few test samples, one column per sample in (y, x, y_hat) format\", grid, global_step=epoch)\n",
        "\n",
        "            writer.flush()\n",
        "\n",
        "    except KeyboardInterrupt:\n",
        "\n",
        "        print('Exiting from training early')"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saving checkpoint for epoch 0\n",
            "New best model found!\n",
            "Saving checkpoint for epoch 20\n",
            "New best model found!\n",
            "Saving checkpoint for epoch 40\n",
            "New best model found!\n",
            "Saving checkpoint for epoch 60\n",
            "New best model found!\n",
            "Saving checkpoint for epoch 80\n",
            "New best model found!\n",
            "Saving checkpoint for epoch 100\n",
            "New best model found!\n",
            "Saving checkpoint for epoch 120\n",
            "New best model found!\n",
            "Saving checkpoint for epoch 140\n",
            "New best model found!\n",
            "Saving checkpoint for epoch 160\n",
            "New best model found!\n",
            "Saving checkpoint for epoch 180\n",
            "Saving checkpoint for epoch 200\n",
            "New best model found!\n",
            "Saving checkpoint for epoch 220\n",
            "New best model found!\n",
            "Saving checkpoint for epoch 240\n",
            "New best model found!\n",
            "Saving checkpoint for epoch 260\n",
            "New best model found!\n",
            "Saving checkpoint for epoch 280\n",
            "Saving checkpoint for epoch 300\n",
            "Saving checkpoint for epoch 320\n",
            "Saving checkpoint for epoch 340\n",
            "New best model found!\n",
            "Saving checkpoint for epoch 360\n",
            "Saving checkpoint for epoch 380\n",
            "New best model found!\n",
            "Saving checkpoint for epoch 400\n",
            "Saving checkpoint for epoch 420\n",
            "New best model found!\n",
            "Saving checkpoint for epoch 440\n",
            "New best model found!\n",
            "Saving checkpoint for epoch 460\n",
            "Saving checkpoint for epoch 480\n",
            "New best model found!\n",
            "Saving checkpoint for epoch 500\n",
            "New best model found!\n",
            "Saving checkpoint for epoch 520\n",
            "New best model found!\n",
            "Saving checkpoint for epoch 540\n",
            "Saving checkpoint for epoch 560\n",
            "New best model found!\n",
            "Saving checkpoint for epoch 580\n",
            "Saving checkpoint for epoch 600\n",
            "Saving checkpoint for epoch 620\n",
            "Saving checkpoint for epoch 640\n",
            "Saving checkpoint for epoch 660\n",
            "Saving checkpoint for epoch 680\n",
            "Saving checkpoint for epoch 700\n",
            "New best model found!\n",
            "Saving checkpoint for epoch 720\n",
            "Saving checkpoint for epoch 740\n",
            "Saving checkpoint for epoch 760\n",
            "Saving checkpoint for epoch 780\n",
            "Saving checkpoint for epoch 800\n",
            "Saving checkpoint for epoch 820\n",
            "Saving checkpoint for epoch 840\n",
            "Saving checkpoint for epoch 860\n",
            "Saving checkpoint for epoch 880\n",
            "Saving checkpoint for epoch 900\n",
            "Saving checkpoint for epoch 920\n",
            "Saving checkpoint for epoch 940\n",
            "Saving checkpoint for epoch 960\n",
            "Saving checkpoint for epoch 980\n",
            "Saving checkpoint for epoch 999\n",
            "New best model found!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6XnXtXHZfSVe",
        "colab": {}
      },
      "source": [
        "writer.close()"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jFrBKHfEM6Nf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 21,
      "outputs": []
    }
  ]
}