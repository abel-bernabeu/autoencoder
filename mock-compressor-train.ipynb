{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "train.ipynb",
   "provenance": [],
   "collapsed_sections": [],
   "toc_visible": true,
   "include_colab_link": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "accelerator": "GPU",
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/abel-bernabeu/autoencoder/blob/master/mock-compressor-train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "myODRbvAf3EK",
    "colab_type": "text"
   },
   "source": [
    "# Description\n",
    "This notebook is a template for training autoencoders for different problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AGvOW4eo2S7Y",
    "colab_type": "text"
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "e8UVNfdFx9iX",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "# Uncomment to unpack the source code\n",
    "#!rm autoencoder -rf && rm -rf autoencoder-master && unzip -q autoencoder-master.zip && mv autoencoder-master/autoencoder/ . && rm autoencoder-master -rf\n",
    "\n",
    "# Uncomment to mount Google Drive\n",
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')\n",
    "\n",
    "# Uncomment to unpack the dataset\n",
    "#!rm data -rf && mkdir -p data && cd data && unzip -q /content/drive/My\\ Drive/archive/2020/aidl/image_dataset_part-a.zip\n",
    "\n",
    "# Uncomment to reset all the checkpoints\n",
    "!rm params -rf && mkdir -p params\n",
    "\n",
    "# Uncomment to delete all the previous TensorBoard runs\n",
    "!rm runs -rf"
   ],
   "execution_count": 86,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rWvnxFWQlPiY",
    "colab_type": "text"
   },
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "TdAi7hZDf3EO",
    "colab_type": "code",
    "colab": {},
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "hparams = {\n",
    "    'batch_size': 8,\n",
    "    'device': 'cuda',    \n",
    "    'train_dataset_size':10,\n",
    "    'test_dataset_size': 10,\n",
    "    'num_epochs': 1000,\n",
    "    'num_workers': 4,\n",
    "    'params' : \"./params/mock-compressor.pt\",\n",
    "    'continue_with_best_model' : False,\n",
    "    'checkpointing_freq' : 20\n",
    "}"
   ],
   "execution_count": 87,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SBBpNtoZf3EY",
    "colab_type": "text",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Dataset transforms"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "TRfQvUDLf3EZ",
    "colab_type": "code",
    "colab": {},
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "import torchvision.transforms as transforms\n",
    "\n",
    "train_input_transform = transforms.Compose([transforms.ToTensor()])\n",
    "train_output_transform = transforms.Compose([transforms.ToTensor()])\n",
    "test_input_transform = transforms.Compose([transforms.ToTensor()])\n",
    "test_output_transform = transforms.Compose([transforms.ToTensor()])"
   ],
   "execution_count": 88,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ntf1Z4_Yf3Eg",
    "colab_type": "text",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "3OKn6N3TbJ84",
    "colab": {},
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import autoencoder.models\n",
    "\n",
    "model = autoencoder.models.MockCompressor(input_width=224, input_height=224)"
   ],
   "execution_count": 89,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kBIbWHHrf3El",
    "colab_type": "text",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Training generics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ysBd-c-DlYA8",
    "colab_type": "text"
   },
   "source": [
    "## TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "AKr9CMXqSDj2",
    "colab": {},
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# Launch TensorBoard\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir runs --port 6007\n",
    "%reload_ext tensorboard"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "THBPV92Kf3Ex",
    "colab_type": "text",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Kick off"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "5EXszz0fSDkK",
    "colab": {},
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torchvision.utils\n",
    "import numpy as np\n",
    "import autoencoder.datasets\n",
    "import datetime\n",
    "import os\n",
    "import math"
   ],
   "execution_count": 91,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "AEuC5t4iM_CC",
    "colab": {}
   },
   "source": [
    "writer = SummaryWriter('')"
   ],
   "execution_count": 92,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "_1IKGDMxCOGL",
    "colab": {}
   },
   "source": [
    "# Create a crops dataset\n",
    "crops = autoencoder.datasets.CropsDataset(\"./data/image_dataset_part-a\", 224, 224, assume_fixed_size=True)\n",
    "\n",
    "# Show a few crops\n",
    "#few_crops = [ transforms.ToTensor()(crop[0]) for crop in [crops[index] for index in range(16)]]\n",
    "#grid = torchvision.utils.make_grid(few_crops, nrow=4)\n",
    "#writer.add_image(\"1) a few crops\", grid)\n",
    "#writer.flush()"
   ],
   "execution_count": 93,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "NaXiCozCEzB0",
    "colab": {}
   },
   "source": [
    "# Random split the original dataset in train, test and discarded datasets\n",
    "train_dataset_size = hparams['train_dataset_size']\n",
    "test_dataset_size = hparams['test_dataset_size']\n",
    "train_crops, test_crops, _ = \\\n",
    "  torch.utils.data.random_split(crops, [ \\\n",
    "    train_dataset_size, \\\n",
    "    test_dataset_size, \\\n",
    "    len(crops) - train_dataset_size - test_dataset_size])"
   ],
   "execution_count": 94,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "L5Rlhxt2SaaX",
    "colab": {}
   },
   "source": [
    "# Wrap the train samples with an XYDimsDataset\n",
    "train_xydims_samples = autoencoder.datasets.XYDimsDataset(train_input_transform, train_output_transform, dataset=train_crops)\n",
    "\n",
    "# Show x from a few train samples\n",
    "#few_train_x = [ sample[0] for sample in [train_xydims_samples[index] for index in range(4)] ]\n",
    "#grid = torchvision.utils.make_grid(few_train_x, nrow=4)\n",
    "#writer.add_image(\"2) x from a few train samples\", grid)\n",
    "#writer.flush()\n",
    "\n",
    "# Show y from a few train samples\n",
    "#few_train_y = [ sample[1] for sample in [train_xydims_samples[index] for index in range(4)] ]\n",
    "#grid = torchvision.utils.make_grid(few_train_y, nrow=4)\n",
    "#writer.add_image(\"3) y from a few train samples\", grid)\n",
    "#writer.flush()"
   ],
   "execution_count": 95,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "qm1qG67AUQx3",
    "colab": {}
   },
   "source": [
    "# Wrap the test samples with an XYDimsDataset\n",
    "test_xydims_samples = autoencoder.datasets.XYDimsDataset(test_input_transform, test_output_transform, dataset=test_crops)\n",
    "\n",
    "# Show x from a few test samples\n",
    "#few_test_x = [ sample[0] for sample in [test_xydims_samples[index] for index in range(4)] ]\n",
    "#grid = torchvision.utils.make_grid(few_test_x, nrow=4)\n",
    "#writer.add_image(\"4) x from a few test samples\", grid)\n",
    "#writer.flush()\n",
    "\n",
    "# Show y from a few train samples\n",
    "#few_test_y = [ sample[1] for sample in [test_xydims_samples[index] for index in range(4)] ]\n",
    "#grid = torchvision.utils.make_grid(few_test_y, nrow=4)\n",
    "#writer.add_image(\"5) y from a few test samples\", grid)\n",
    "#writer.flush()"
   ],
   "execution_count": 96,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "-MxSoKSUnXgS",
    "colab": {}
   },
   "source": [
    "# Create data loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_xydims_samples, batch_size=hparams['batch_size'], shuffle=True, num_workers=hparams['num_workers'])\n",
    "test_loader = torch.utils.data.DataLoader(test_xydims_samples, batch_size=hparams['batch_size'], shuffle=False, num_workers=hparams['num_workers'])"
   ],
   "execution_count": 97,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "jOW0NNlhfHA5",
    "colab": {}
   },
   "source": [
    "# Some auxilary functions for the training loop\n",
    "def train_epoch(train_loader, model, optimizer, criterion, hparams):\n",
    "    np.random.seed(datetime.datetime.now().microsecond)\n",
    "    model.train()\n",
    "    device = hparams['device']\n",
    "    losses = []\n",
    "    for data, target, _, _ in train_loader:\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "    return np.mean(losses)\n",
    "\n",
    "def test_epoch(test_loader, model, criterion, hparams):\n",
    "    np.random.seed(0)\n",
    "    model.eval()\n",
    "    device = hparams['device']\n",
    "    eval_losses = []\n",
    "    with torch.no_grad():\n",
    "        for data, target, _, _ in test_loader:\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "            output = model(data)\n",
    "            eval_losses.append(criterion(output, target).item())\n",
    "    return np.mean(eval_losses)\n",
    "\n",
    "def inference(model, inputs_list):\n",
    "    \"\"\"\n",
    "    Do an inference with the model for each input tensor from the provided list and\n",
    "    return a list with the inference results\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    for x in inputs_list:\n",
    "        num_channels = x.shape[0]\n",
    "        height = x.shape[1]\n",
    "        width = x.shape[2]\n",
    "        single_element_batch = x.clone().detach().reshape(1, num_channels, height, width)\n",
    "        single_element_batch = single_element_batch.to(hparams['device'])\n",
    "        model.to(hparams['device'])\n",
    "        model.eval()\n",
    "        output = model(single_element_batch)\n",
    "        output = output.reshape(num_channels, height, width)\n",
    "        result.append(output)\n",
    "    return result\n",
    "\n",
    "def psnr(max_i, mse):\n",
    "  return 20*math.log10(max_i) - 10*math.log10(mse)  "
   ],
   "execution_count": 98,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "Z7U6yTCe7xqe",
    "colab": {}
   },
   "source": [
    "# Move few_test_x to the same device where the inferences will be left\n",
    "for index in range(len(few_test_x)):\n",
    "  few_test_x[index] = few_test_x[index].to(hparams['device'])\n",
    "\n",
    "# Move few_test_y to the same device where the inferences will be left\n",
    "for index in range(len(few_train_y)):\n",
    "  few_train_y[index] = few_train_y[index].to(hparams['device'])\n",
    "\n",
    "# Move few_train_x to the same device where the inferences will be left\n",
    "for index in range(len(few_train_x)):\n",
    "  few_train_x[index] = few_train_x[index].to(hparams['device'])\n",
    "\n",
    "# Move few_train_y to the same device where the inferences will be left\n",
    "for index in range(len(few_test_y)):\n",
    "  few_test_y[index] = few_test_y[index].to(hparams['device'])"
   ],
   "execution_count": 99,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "UF02sWDZf3Fa",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "# Instantiate optimizer and loss\n",
    "optimizer = optim.Adam(model.parameters(), weight_decay=1e-4)\n",
    "criterion = nn.MSELoss()"
   ],
   "execution_count": 100,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "yWsFbEZ5f3Fd",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "# Move model to device\n",
    "model = model.to(hparams['device'])"
   ],
   "execution_count": 101,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "K-TS737bhbTg",
    "colab": {}
   },
   "source": [
    "# Restore previous checkpoint or create new one from scratch\n",
    "if os.path.isfile(hparams['params']):\n",
    "    print(\"Restoring from previous checkpoint\")\n",
    "    checkpoint = torch.load(hparams['params'])    \n",
    "else:\n",
    "    checkpoint = {\n",
    "        'best_train_loss': None,\n",
    "        'best_epoch' : None,\n",
    "        'best_model': None,\n",
    "        'last_epoch' : -1,\n",
    "        'last_model': model.state_dict(),\n",
    "        'optimizer' : optimizer.state_dict()\n",
    "    }\n",
    "\n",
    "# Load model and optimizer from the checkpoint\n",
    "model.load_state_dict(checkpoint['last_model'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "\n",
    "# Run a number of training epochs\n",
    "start = checkpoint['last_epoch'] + 1\n",
    "end = hparams['num_epochs']\n",
    "\n",
    "if start < end - 1 or checkpoint['best_train_loss'] is None:\n",
    "    \n",
    "    try:\n",
    "\n",
    "        for epoch in range(start, end):\n",
    "\n",
    "            train_loss = train_epoch(train_loader, model, optimizer, criterion, hparams)            \n",
    "            test_loss = test_epoch(test_loader, model, criterion, hparams)\n",
    "\n",
    "            # Log the losses\n",
    "            writer.add_scalar(\"train_loss\", train_loss, global_step=epoch)\n",
    "            writer.add_scalar(\"test_loss\", test_loss, global_step=epoch)\n",
    "\n",
    "            # Log PSNRs\n",
    "            train_psnr = psnr(1, train_loss)\n",
    "            test_psnr = psnr(1, test_loss)\n",
    "            writer.add_scalar(\"train_psnr\", train_psnr, global_step=epoch)\n",
    "            writer.add_scalar(\"test_psnr\", test_psnr, global_step=epoch)\n",
    "\n",
    "            if checkpoint['best_train_loss'] is None or train_loss < checkpoint['best_train_loss']:\n",
    "\n",
    "                print('New best model found!')\n",
    "\n",
    "                # Update best model in the checkpoint\n",
    "                checkpoint['best_train_loss'] = train_loss\n",
    "                checkpoint['best_epoch'] = epoch\n",
    "                checkpoint['best_model'] = model.state_dict()\n",
    "\n",
    "                # Show inferences with a few training samples,\n",
    "                # one column per sample in (y, x, y_hat) format\n",
    "                few_train_y_hat = inference(model, few_train_x)\n",
    "                grid = torchvision.utils.make_grid(few_train_y + few_train_x + few_train_y_hat, nrow=4)\n",
    "                writer.add_image(tag='train', img_tensor=grid, global_step=epoch)\n",
    "\n",
    "                # Show inferences with a few test samples,\n",
    "                # one column per sample in (y, x, y_hat) format\n",
    "                few_test_y_hat = inference(model, few_test_x)\n",
    "                grid = torchvision.utils.make_grid(few_test_y + few_test_x + few_test_y_hat, nrow=4)\n",
    "                writer.add_image(tag='test', img_tensor=grid, global_step=epoch)\n",
    "\n",
    "                writer.flush()\n",
    "            \n",
    "            if epoch == hparams['num_epochs'] - 1 or epoch % hparams['checkpointing_freq'] == 0:\n",
    "\n",
    "                print('Saving checkpoint at epoch ' + str(epoch))\n",
    "\n",
    "                # Update last model and optimizer in the checkpoint\n",
    "                checkpoint['last_epoch'] = epoch\n",
    "                checkpoint['last_model'] = model.state_dict()\n",
    "                checkpoint['optimizer'] = optimizer.state_dict()\n",
    "\n",
    "                torch.save(checkpoint, hparams['params'])\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "\n",
    "        print('Exiting from training early')"
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}