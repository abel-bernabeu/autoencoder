{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "twitter_compressor_train.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abel-bernabeu/autoencoder/blob/master/twitter_compressor_train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "myODRbvAf3EK",
        "colab_type": "text"
      },
      "source": [
        "# Description\n",
        "\n",
        "This notebook is an implementation of the enconder/decoder deep learning architecture from \"Lossy image compression with compression autoencoders\", by \n",
        "Lucas Theis, Wenzhe Shi, Andrew Cunningham & Ferenc Husz, published in 2017\n",
        "(https://arxiv.org/pdf/1703.00395v1.pdf).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AGvOW4eo2S7Y",
        "colab_type": "text"
      },
      "source": [
        "# Baseline model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vi0IGZoiE-CA",
        "colab_type": "text"
      },
      "source": [
        "Our baseline model focuses on training the proposed deep neural network with the maximum possible accuracy ignoring the quantization and the entropic coding. This model achieves no compression at all, but it is used for three purposes:\n",
        "\n",
        "- transfer the learned transform to more advance models with quantization\n",
        "\n",
        "- set an upper bound on accuracy (**43 db**)\n",
        "\n",
        "- give an estimation of how long it takes to train a compression model (**4 days on a Tesla P100**)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e8UVNfdFx9iX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Uncomment to install tensorboard\n",
        "#!pip install tensorboard\n",
        "\n",
        "# Uncomment to mount Google Drive\n",
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive/')\n",
        "#!ln -s  /content/drive/My\\ Drive/archive/2020/aidl/ share\n",
        "\n",
        "# Uncomment to unpack the source code\n",
        "#!rm autoencoder -rf && \\\n",
        "# rm -rf autoencoder-master && \\\n",
        "# unzip -q share/autoencoder-master.zip && \\\n",
        "# mv autoencoder-master/autoencoder/ . && \\\n",
        "# rm autoencoder-master -rf\n",
        "\n",
        "# Uncomment to unpack the dataset\n",
        "#!rm data -rf && mkdir -p data && cd data && \\\n",
        "# unzip -q ../share/image_dataset_part-a.zip\n",
        "\n",
        "# Uncomment to reset all the checkpoints\n",
        "#!rm share/twitter-compressor/twitter-compressor.pt -rf\n",
        "\n",
        "# Uncomment to delete all the previous TensorBoard runs\n",
        "#!rm share/twitter-compressor/runs -rf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rWvnxFWQlPiY",
        "colab_type": "text"
      },
      "source": [
        "## Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TdAi7hZDf3EO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hparams = {\n",
        "    'batch_size' : 32,\n",
        "    'device' : 'cuda',\n",
        "    'block_width' : 224,\n",
        "    'block_height' : 224,\n",
        "    'train_dataset_size' : 5000,\n",
        "    'test_dataset_size' : 500,\n",
        "    'num_epochs' : 20000,\n",
        "    'num_workers' : 4,\n",
        "    'params' : \"share/twitter-compressor/twitter-compressor.pt\",\n",
        "    'continue_with_best_model' : False,\n",
        "    'tensorboard_runs' : 'share/twitter-compressor/runs/',\n",
        "    'checkpointing_freq' : 10\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ntf1Z4_Yf3Eg",
        "colab_type": "text"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3OKn6N3TbJ84",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import autoencoder.models.quantization\n",
        "\n",
        "\n",
        "class CompressionAutoencoder(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, quantization=False):\n",
        "        super(CompressionAutoencoder, self).__init__()\n",
        "        self.encoder = None\n",
        "        if quantization:\n",
        "            self.quantize = autoencoder.models.Quantize()\n",
        "            self.dequantize = autoencoder.models.Dequantize()\n",
        "        else:\n",
        "            self.quantize = None\n",
        "            self.dequantize = None\n",
        "        self.decoder = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = self.encoder(x)\n",
        "        y = self.decoder(h)\n",
        "        yp = torch.nn.functional.hardtanh(y)\n",
        "        return (yp + 1) * 0.5\n",
        "\n",
        "\n",
        "class TwitterEncoder(torch.nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(TwitterEncoder, self).__init__()\n",
        "\n",
        "        self.block1 = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=5, stride=2, padding=2, padding_mode='replicate'),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.LeakyReLU())\n",
        "  \n",
        "        self.block2 = nn.Sequential(\n",
        "            nn.Conv2d(64, 128, kernel_size=5, stride=2, padding=2, padding_mode='replicate'),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.LeakyReLU())\n",
        "\n",
        "        self.block3 = nn.Sequential(\n",
        "            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1, padding_mode='replicate'),\n",
        "            nn.BatchNorm2d(128),            \n",
        "            nn.LeakyReLU())\n",
        "\n",
        "        self.block4 = nn.Sequential(\n",
        "            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1, padding_mode='replicate'),\n",
        "            nn.BatchNorm2d(128),            \n",
        "            nn.LeakyReLU())\n",
        "\n",
        "        self.block5 = nn.Sequential(\n",
        "            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1, padding_mode='replicate'),\n",
        "            nn.BatchNorm2d(128),            \n",
        "            nn.LeakyReLU())\n",
        "\n",
        "        self.block6 = nn.Sequential(\n",
        "            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1, padding_mode='replicate'),\n",
        "            nn.BatchNorm2d(128),            \n",
        "            nn.LeakyReLU())\n",
        "\n",
        "        self.block7 = nn.Sequential(\n",
        "            nn.Conv2d(128, 96, kernel_size=5, stride=2, padding=2, padding_mode='replicate'))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.block1(x)\n",
        "        x = self.block2(x)\n",
        "        x = self.block3(x) + x\n",
        "        x = self.block4(x) + x\n",
        "        x = self.block5(x) + x\n",
        "        x = self.block6(x) + x\n",
        "        x = self.block7(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class TwitterDecoder(torch.nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(TwitterDecoder, self).__init__()\n",
        "\n",
        "        self.block1 = nn.Sequential(\n",
        "            nn.Conv2d(96, 512*4, kernel_size=3, stride=1, padding=1, padding_mode='replicate'),\n",
        "            nn.PixelShuffle(2),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.LeakyReLU())\n",
        "\n",
        "        self.block2 = nn.Sequential(\n",
        "            nn.Conv2d(512, 128, kernel_size=3, stride=1, padding=1, padding_mode='replicate'),\n",
        "            nn.BatchNorm2d(128),            \n",
        "            nn.LeakyReLU())\n",
        "\n",
        "        self.block3 = nn.Sequential(\n",
        "            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1, padding_mode='replicate'),\n",
        "            nn.BatchNorm2d(128),            \n",
        "            nn.LeakyReLU())\n",
        "\n",
        "        self.block4 = nn.Sequential(\n",
        "            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1, padding_mode='replicate'),\n",
        "            nn.BatchNorm2d(128),                        \n",
        "            nn.LeakyReLU())\n",
        "\n",
        "        self.block5 = nn.Sequential(\n",
        "            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1, padding_mode='replicate'),\n",
        "            nn.BatchNorm2d(128),                        \n",
        "            nn.LeakyReLU())\n",
        "\n",
        "        self.block6 = nn.Sequential(\n",
        "            nn.Conv2d(128, 256*4, kernel_size=3, stride=1, padding=1, padding_mode='replicate'),          \n",
        "            nn.PixelShuffle(2),\n",
        "            nn.BatchNorm2d(256),                        \n",
        "            nn.LeakyReLU())\n",
        "\n",
        "        self.block7 = nn.Sequential(\n",
        "            nn.Conv2d(256, 3*4, kernel_size=3, stride=1, padding=1, padding_mode='replicate'),\n",
        "            nn.PixelShuffle(2))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.block1(x)\n",
        "        x = self.block2(x) \n",
        "        x = self.block3(x) + x\n",
        "        x = self.block4(x) + x\n",
        "        x = self.block5(x) + x\n",
        "        x = self.block6(x)\n",
        "        x = self.block7(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class TwitterCompressor(CompressionAutoencoder):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(CompressionAutoencoder, self).__init__()\n",
        "        self.encoder = TwitterEncoder()\n",
        "        self.decoder = TwitterDecoder()\n",
        "\n",
        "\n",
        "model = TwitterCompressor()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ysBd-c-DlYA8",
        "colab_type": "text"
      },
      "source": [
        "## TensorBoard"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "AKr9CMXqSDj2",
        "colab": {}
      },
      "source": [
        "# Launch TensorBoard\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir share/twitter-compressor/runs/ --port 6011\n",
        "%reload_ext tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "THBPV92Kf3Ex",
        "colab_type": "text"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5EXszz0fSDkK",
        "colab": {}
      },
      "source": [
        "import torchvision.transforms as transforms\n",
        "import torch.optim as optim\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import torchvision.utils\n",
        "import numpy as np\n",
        "import autoencoder.datasets\n",
        "import datetime\n",
        "import os\n",
        "import math"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xXbtDD_PqYcm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_dataloaders(hparams):\n",
        "\n",
        "    # Set the seed to a known state for reproducibility of the training\n",
        "    torch.manual_seed(0x1234)\n",
        "\n",
        "    # Create a crops dataset\n",
        "    crops = autoencoder.datasets.CropsDataset(\n",
        "        \"./data/image_dataset_part-a\",\n",
        "        block_width=hparams['block_width'],\n",
        "        block_height=hparams['block_height'],\n",
        "        assume_fixed_size=False)\n",
        "\n",
        "    # Random split the original dataset in train, test and discarded datasets\n",
        "    train_dataset_size = hparams['train_dataset_size']\n",
        "    test_dataset_size = hparams['test_dataset_size']\n",
        "    train_crops, test_crops, _ = \\\n",
        "      torch.utils.data.random_split(crops, [ \\\n",
        "        train_dataset_size, \\\n",
        "        test_dataset_size, \\\n",
        "        len(crops) - train_dataset_size - test_dataset_size])\n",
        "      \n",
        "    # Create the dataset transforms\n",
        "    train_input_transform = transforms.Compose([transforms.ToTensor()])\n",
        "    train_output_transform = transforms.Compose([transforms.ToTensor()])\n",
        "    test_input_transform = transforms.Compose([transforms.ToTensor()])\n",
        "    test_output_transform = transforms.Compose([transforms.ToTensor()])  \n",
        "      \n",
        "    # Wrap the train samples with an XYDimsDataset\n",
        "    train_xydims_samples = autoencoder.datasets.XYDimsDataset(train_input_transform, train_output_transform, dataset=train_crops)\n",
        "\n",
        "    # Wrap the test samples with an XYDimsDataset\n",
        "    test_xydims_samples = autoencoder.datasets.XYDimsDataset(test_input_transform, test_output_transform, dataset=test_crops)\n",
        "\n",
        "    # Create data loaders\n",
        "    train_loader = torch.utils.data.DataLoader(train_xydims_samples, batch_size=hparams['batch_size'], shuffle=True, num_workers=hparams['num_workers'])\n",
        "    test_loader = torch.utils.data.DataLoader(test_xydims_samples, batch_size=hparams['batch_size'], shuffle=False, num_workers=hparams['num_workers'])\n",
        "\n",
        "    # Pick a few train samples\n",
        "    few_train_x = [ sample[0] for sample in [train_xydims_samples[index] for index in range(4)] ]\n",
        "    few_train_y = [ sample[1] for sample in [train_xydims_samples[index] for index in range(4)] ]\n",
        "\n",
        "    # Move few_train_x to the same device where the inferences will be left\n",
        "    for index in range(len(few_train_x)):\n",
        "      few_train_x[index] = few_train_x[index].to(hparams['device'])\n",
        "\n",
        "    # Move few_test_y to the same device where the inferences will be left\n",
        "    for index in range(len(few_train_y)):\n",
        "      few_train_y[index] = few_train_y[index].to(hparams['device'])\n",
        "\n",
        "    # Pick a few test samples\n",
        "    few_test_x = [ sample[0] for sample in [test_xydims_samples[index] for index in range(4)] ]\n",
        "    few_test_y = [ sample[1] for sample in [test_xydims_samples[index] for index in range(4)] ]\n",
        "\n",
        "    # Move few_test_x to the same device where the inferences will be left\n",
        "    for index in range(len(few_test_x)):\n",
        "      few_test_x[index] = few_test_x[index].to(hparams['device'])\n",
        "\n",
        "    # Move few_train_y to the same device where the inferences will be left\n",
        "    for index in range(len(few_test_y)):\n",
        "      few_test_y[index] = few_test_y[index].to(hparams['device'])\n",
        "\n",
        "    return train_loader, test_loader, few_train_x, few_train_y, few_test_x, few_test_y\n",
        "\n",
        "train_loader, test_loader, few_train_x, few_train_y, few_test_x, few_test_y = create_dataloaders(hparams)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5cyzSi8YlnvG",
        "colab_type": "text"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jOW0NNlhfHA5",
        "colab": {}
      },
      "source": [
        "# Some auxilary functions for the training loop\n",
        "def train_epoch(train_loader, model, optimizer, criterion, hparams):\n",
        "    np.random.seed(datetime.datetime.now().microsecond)\n",
        "    model.train()\n",
        "    device = hparams['device']\n",
        "    losses = []\n",
        "    for data, target, _, _ in train_loader:\n",
        "        data = data.to(device)\n",
        "        target = target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        losses.append(loss.item())\n",
        "    return np.mean(losses)\n",
        "\n",
        "def test_epoch(test_loader, model, criterion, hparams):\n",
        "    np.random.seed(0)\n",
        "    model.eval()\n",
        "    device = hparams['device']\n",
        "    eval_losses = []\n",
        "    with torch.no_grad():\n",
        "        for data, target, _, _ in test_loader:\n",
        "            data = data.to(device)\n",
        "            target = target.to(device)\n",
        "            output = model(data)\n",
        "            eval_losses.append(criterion(output, target).item())\n",
        "    return np.mean(eval_losses)\n",
        "\n",
        "def inference(model, inputs_list, hparams):\n",
        "    \"\"\"\n",
        "    Do an inference with the model for each input tensor from the provided list and\n",
        "    return a list with the inference results\n",
        "    \"\"\"\n",
        "    result = []\n",
        "    for x in inputs_list:\n",
        "        num_channels = x.shape[0]\n",
        "        height = x.shape[1]\n",
        "        width = x.shape[2]\n",
        "        single_element_batch = x.clone().detach().reshape(1, num_channels, height, width)\n",
        "        single_element_batch = single_element_batch.to(hparams['device'])\n",
        "        model.to(hparams['device'])\n",
        "        model.eval()\n",
        "        output = model(single_element_batch)\n",
        "        output = output.reshape(num_channels, height, width)\n",
        "        result.append(output)\n",
        "    return result\n",
        "\n",
        "def psnr(mean_square_normalized_error):\n",
        "  max_i = 255.0\n",
        "  mse = max(mean_square_normalized_error, 1e-10) * max_i * max_i\n",
        "  return 20*math.log10(max_i) - 10*math.log10(mse)\n",
        "\n",
        "def psnr2(mean_square_normalized_error):\n",
        "  return - 10*math.log10(max(mean_square_normalized_error, 1e-10))\n",
        "\n",
        "def train(hparams, model, train_loader, test_loader, few_train_x, few_train_y, few_test_x, few_test_y):\n",
        "\n",
        "    # Create the summary writer\n",
        "    writer = SummaryWriter(hparams['tensorboard_runs'])\n",
        "\n",
        "    # Instantiate optimizer and loss\n",
        "    optimizer = optim.Adam(model.parameters())\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    # Move model to device\n",
        "    model = model.to(hparams['device'])\n",
        "\n",
        "    # Restore previous checkpoint or create new one from scratch\n",
        "    if os.path.isfile(hparams['params']):\n",
        "        print(\"Restoring from previous checkpoint\")\n",
        "        checkpoint = torch.load(hparams['params'])    \n",
        "    else:\n",
        "        checkpoint = {\n",
        "            'best_train_loss': None,\n",
        "            'best_epoch' : None,\n",
        "            'best_model': None,\n",
        "            'last_epoch' : -1,\n",
        "            'last_model': model.state_dict(),\n",
        "            'optimizer' : optimizer.state_dict()\n",
        "        }\n",
        "\n",
        "    # Load model and optimizer from the checkpoint\n",
        "    model.load_state_dict(checkpoint['last_model'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "\n",
        "    # Run a number of training epochs\n",
        "    start = checkpoint['last_epoch'] + 1\n",
        "    end = hparams['num_epochs']\n",
        "\n",
        "    if start < end - 1 or checkpoint['best_train_loss'] is None:\n",
        "        \n",
        "        try:\n",
        "\n",
        "            for epoch in range(start, end):\n",
        "\n",
        "                train_loss = train_epoch(train_loader, model, optimizer, criterion, hparams)            \n",
        "                test_loss = test_epoch(test_loader, model, criterion, hparams)\n",
        "\n",
        "                # Log losses\n",
        "                writer.add_scalar(\"train_loss\", train_loss, global_step=epoch)\n",
        "                writer.add_scalar(\"test_loss\", test_loss, global_step=epoch)\n",
        "\n",
        "                # Log PSNRs\n",
        "                train_psnr = psnr2(train_loss)\n",
        "                test_psnr = psnr2(test_loss)\n",
        "                writer.add_scalar(\"train_psnr\", train_psnr, global_step=epoch)\n",
        "                writer.add_scalar(\"test_psnr\", test_psnr, global_step=epoch)\n",
        "\n",
        "                if checkpoint['best_train_loss'] is None or train_loss < checkpoint['best_train_loss']:\n",
        "\n",
        "                    print('New best model found!')\n",
        "\n",
        "                    # Update best model in the checkpoint\n",
        "                    checkpoint['best_train_loss'] = train_loss\n",
        "                    checkpoint['best_epoch'] = epoch\n",
        "                    checkpoint['best_model'] = model.state_dict()\n",
        "\n",
        "                    # Show inferences with a few training samples,\n",
        "                    # one column per sample in (y, x, y_hat) format\n",
        "                    few_train_y_hat = inference(model, few_train_x, hparams)\n",
        "                    grid = torchvision.utils.make_grid(few_train_y + few_train_x + few_train_y_hat, nrow=4)\n",
        "                    writer.add_image(tag='train', img_tensor=grid, global_step=epoch)\n",
        "\n",
        "                    # Show inferences with a few test samples,\n",
        "                    # one column per sample in (y, x, y_hat) format\n",
        "                    few_test_y_hat = inference(model, few_test_x, hparams)\n",
        "                    grid = torchvision.utils.make_grid(few_test_y + few_test_x + few_test_y_hat, nrow=4)\n",
        "                    writer.add_image(tag='test', img_tensor=grid, global_step=epoch)\n",
        "\n",
        "                    writer.flush()\n",
        "                \n",
        "                if epoch == hparams['num_epochs'] - 1 or epoch % hparams['checkpointing_freq'] == 0:\n",
        "\n",
        "                    print('Saving checkpoint at epoch ' + str(epoch))\n",
        "\n",
        "                    # Update last model and optimizer in the checkpoint\n",
        "                    checkpoint['last_epoch'] = epoch\n",
        "                    checkpoint['last_model'] = model.state_dict()\n",
        "                    checkpoint['optimizer'] = optimizer.state_dict()\n",
        "\n",
        "                    torch.save(checkpoint, hparams['params'])\n",
        "\n",
        "        except KeyboardInterrupt:\n",
        "\n",
        "            print('Exiting from training early')\n",
        "\n",
        "    writer.close()\n",
        "\n",
        "train(hparams=hparams, \\\n",
        "      model=model, \\\n",
        "      train_loader=train_loader, \\\n",
        "      test_loader=test_loader, \\\n",
        "      few_train_x=few_train_x, few_train_y=few_train_y, \\\n",
        "      few_test_x=few_test_x, few_test_y=few_test_y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UynFM6TYEL12",
        "colab_type": "text"
      },
      "source": [
        "# Uniform quantization to 8 bits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DJEvb7N22tNR",
        "colab_type": "text"
      },
      "source": [
        "## Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xBTpKybm2yAp",
        "colab": {}
      },
      "source": [
        "hparams = {\n",
        "    'batch_size' : 32,\n",
        "    'device' : 'cuda',\n",
        "    'block_width' : 224,\n",
        "    'block_height' : 224,\n",
        "    'train_dataset_size' : 5000,\n",
        "    'test_dataset_size' : 500,\n",
        "    'num_epochs' : 20000,\n",
        "    'num_workers' : 4,\n",
        "    'params' : \"share/twitter-compressor/uniform-quant-twitter-compressor.pt\",\n",
        "    'continue_with_best_model' : False,\n",
        "    'tensorboard_runs' : 'share/uniform-quant-twitter-compressor/runs/',\n",
        "    'checkpointing_freq' : 10\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3aE-Rr3v2pdD",
        "colab_type": "text"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RRdkdcGBGiu6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class QuantizingCompressionAutoencoder(torch.nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(QuantizingCompressionAutoencoder, self).__init__()\n",
        "        self.encoder = None\n",
        "        self.quantize = autoencoder.models.Quantize()\n",
        "        self.dequantize = autoencoder.models.Dequantize()\n",
        "        self.decoder = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = self.encoder(x)\n",
        "        hq = self.quantize(h)\n",
        "        hd = self.dequantize(hq)\n",
        "        y = self.decoder(hd)\n",
        "        yp = torch.nn.functional.hardtanh(y)\n",
        "        return (yp + 1) * 0.5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZW_nmDLzGj0w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class QuantizingTwitterCompressor(QuantizingCompressionAutoencoder):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(QuantizingTwitterCompressor, self).__init__()\n",
        "        self.encoder = TwitterEncoder()\n",
        "        self.decoder = TwitterDecoder()\n",
        "\n",
        "model = QuantizingTwitterCompressor()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gtxmt1Lv4wBl",
        "colab_type": "text"
      },
      "source": [
        "## TensorBoard"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4sxUAGmi43tT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Launch TensorBoard\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir share/uniform-quant-twitter-compressor/runs/ --port 6020\n",
        "%reload_ext tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L0wePJAW5bpC",
        "colab_type": "text"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C0Fv9Pdr5d4R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_loader, test_loader, few_train_x, few_train_y, few_test_x, few_test_y = create_dataloaders(hparams)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "deyZjDZx5jfa",
        "colab_type": "text"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XCm6ZWG05exL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train(hparams, model, train_loader, test_loader, few_train_x, few_train_y, few_test_x, few_test_y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YnmZV6D68-43",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}